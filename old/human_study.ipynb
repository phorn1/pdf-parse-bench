{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-10T09:17:24.246258Z",
     "start_time": "2025-09-10T09:17:23.932260Z"
    }
   },
   "source": [
    "import json\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import os\n",
    "\n",
    "# Set up the path to the human study results\n",
    "base_path = \"/Users/piushorn/PycharmProjects/pdf-to-text-benchmark/artifacts/runs/human_study\"\n",
    "\n",
    "# Find all eval_formula_results.json files\n",
    "json_files = glob.glob(f\"{base_path}/*/mistral/eval_formula_results.json\")\n",
    "print(f\"Found {len(json_files)} JSON files to process\")\n",
    "\n",
    "# Initialize data storage for each LLM judge model\n",
    "llm_scores = defaultdict(list)\n",
    "\n",
    "# Process each JSON file\n",
    "for json_file in json_files:\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract scores for each LLM judge model\n",
    "    for formula_eval in data:\n",
    "        for llm_eval in formula_eval['llm_evals']:\n",
    "            judge_model = llm_eval['judge_model']\n",
    "            score = llm_eval['score']\n",
    "            llm_scores[judge_model].append(score)\n",
    "    \n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 JSON files to process\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "a4scu31cnft",
   "source": [
    "# Analysis: Formulas with perfect scores from both CDM (1.0) and GPT-5 (10.0)\n",
    "print(\"Analysis: Formulas with Perfect Scores from Both CDM and GPT-5\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize counters and storage\n",
    "perfect_both_count = 0\n",
    "cdm_perfect_count = 0\n",
    "gpt5_perfect_count = 0\n",
    "total_formulas = 0\n",
    "perfect_both_formulas = []\n",
    "\n",
    "# Process each JSON file again to collect CDM and GPT-5 scores per formula\n",
    "for json_file in json_files:\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    for formula_eval in data:\n",
    "        total_formulas += 1\n",
    "        formula_number = formula_eval['formula_number']\n",
    "        \n",
    "        # Get CDM score\n",
    "        cdm_score = formula_eval.get('cdm_eval', {}).get('score', None)\n",
    "        \n",
    "        # Get GPT-5 score\n",
    "        gpt5_score = None\n",
    "        for llm_eval in formula_eval['llm_evals']:\n",
    "            if llm_eval['judge_model'] == 'gpt-5':\n",
    "                gpt5_score = llm_eval['score']\n",
    "                break\n",
    "        \n",
    "        # Count perfect scores\n",
    "        if cdm_score == 1.0:\n",
    "            cdm_perfect_count += 1\n",
    "        \n",
    "        if gpt5_score == 10.0:\n",
    "            gpt5_perfect_count += 1\n",
    "        \n",
    "        # Check if both have perfect scores\n",
    "        if cdm_score == 1.0 and gpt5_score == 10.0:\n",
    "            perfect_both_count += 1\n",
    "            perfect_both_formulas.append({\n",
    "                'file': json_file.split('/')[-3],  # Extract folder number (000, 001, etc.)\n",
    "                'formula_number': formula_number,\n",
    "                'ground_truth': formula_eval['ground_truth_formula'],\n",
    "                'extracted': formula_eval['extracted_formula'],\n",
    "                'formula_type': formula_eval['formula_type']\n",
    "            })\n",
    "\n",
    "print(f\"Total formulas analyzed: {total_formulas}\")\n",
    "print(f\"CDM perfect scores (1.0): {cdm_perfect_count} ({cdm_perfect_count/total_formulas*100:.1f}%)\")\n",
    "print(f\"GPT-5 perfect scores (10.0): {gpt5_perfect_count} ({gpt5_perfect_count/total_formulas*100:.1f}%)\")\n",
    "print(f\"Both CDM=1.0 AND GPT-5=10.0: {perfect_both_count} ({perfect_both_count/total_formulas*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nOverlap Analysis:\")\n",
    "print(f\"Of CDM perfect scores, {perfect_both_count}/{cdm_perfect_count} ({perfect_both_count/cdm_perfect_count*100:.1f}%) also got GPT-5 perfect\")\n",
    "print(f\"Of GPT-5 perfect scores, {perfect_both_count}/{gpt5_perfect_count} ({perfect_both_count/gpt5_perfect_count*100:.1f}%) also got CDM perfect\")\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T09:17:52.147361Z",
     "start_time": "2025-09-10T09:17:52.134938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis: Formulas with Perfect Scores from Both CDM and GPT-5\n",
      "============================================================\n",
      "Total formulas analyzed: 390\n",
      "CDM perfect scores (1.0): 245 (62.8%)\n",
      "GPT-5 perfect scores (10.0): 251 (64.4%)\n",
      "Both CDM=1.0 AND GPT-5=10.0: 217 (55.6%)\n",
      "\n",
      "Overlap Analysis:\n",
      "Of CDM perfect scores, 217/245 (88.6%) also got GPT-5 perfect\n",
      "Of GPT-5 perfect scores, 217/251 (86.5%) also got CDM perfect\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
